
Konvencionalne tehnike strojnog učenja su bile limitirane u procesiranju podataka u njihovom sirovom obliku (slike, tekst, ...).
Desetljećima su se konstruirali sustavi za prepoznavanje uzoraka ili ostali sustavi za ekstrakciju značajka koji bi tada transformirali te sirove podatke u pripadnu reprezentaciju ili neki vektor značajki iz kojih bi se moglo klasificirati nešto.

Glavno "oružje" dubokog učenja su umjetne neuronske mreže, koje se savršeno uklapaju u princip rada metoda dubokih učenja. Te metode se zasnivaju na tome da imaju više razina reprezentacije, a svaka ta razina ima jednostavne nelinearne module koji transformiraju podatke dobivene iz prethodne razine, s time da u prvoj razini, na ulazu modela dubokog učenja kao podatke model dobiva sirove podatke, dakle sliku, tekst, zvuk i slično, što nije prethodno bilo moguće kod standardnih metoda strojnog učenja.

U slučaju slika, razni slojevi odrađuju različit posao te na taj način simuliraju postupak ekstrakcije značajki kod konvencionalnih metoda strojnog učenja. Na primjer, prvih nekoliko slojeva neke neuronske mreže prepoznaju pozicije nekih rubova i njihov raspored, neki dublji slojevi mogu slagati te prethodne značajke te na taj način napraviti detekciju objekata.

Upravo u tim nelinarnim jedinicama je najveća prednost dubokog učenja, sama činjenica što se značajke i veze između njih pronalaze kroz skrivene slojeve, to uklanja puno posla prije samog odabira modela kojim će se klasificirati neka slika (na primjer). Te metode su dokazale da su trenutno najbolje u nekim područjima, a jedno od područja u kojem su metode dubokog učenja najizraženije jest računalni vid. Na najvećem natjecanju u području računalnog vida ImageNet, upravo modeli dubokih neuronskih mreža su zabilježili najbolje rezultate (REFERENCA)

Najčešći oblik strojnog (ujedno i dubokog) učenja jest nadgledano učenje. Ono funkcionira na način da postoji skup označenih podataka nad kojim se trenira model. Model unutar svojih jedinica ima težine koje se namještaju ovisno o ulaznom podatku i njegovoj oznaci. Postupak kojim se to ostvaruje zove se stohastički gradijentni spust.

Problem je što su te jedinice unutar skrivenih slojeva kod standardnih metoda strojnog učenja linearne te zbog toga postupak izvlačenja značajki treba biti vrlo dobar kako bi se dobio povoljan rezultat, što iziskuje puno truda i vremena. Kod arhitektura dubokog učenja višeslojni skupovi jednostavnih nelineranih jedinica omogućavaju prepoznavanje između sitnih razlika pojedinih objekata.

Konvolucijske neuronske mreže

- procesiraju podatke u obliku višestrukih polja, npr. slika u boji se sastoji od tri 2D polja koja sadržavaju intenzitet piksela u tri kanala boje

- Pooling sloj (Sloj sažimanja) je zadužen za spajanje semantički sličnih značajki u jednu. Tipična pooling jedinica računa maksimum lokalnih dijelova koji su pomaknuti za više od jednog stupca/retka i time smanjuju dimenzionalnost reprezentacije i stvaraju optornost na  male pomake i distorzije.

- Konvolucijske neuronske mreže su poprimilie veliki rast nakon već spomenutog ImageNet natjecanja iz 2012. godine kada su duboke neuronske mreže gotovo dvostruko smanjile greške u usporedbi s ostalim pristupima (PROC ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 25)

- u tome su pomogle grafičke kartice ,ReLU prijenosne funkcije, nova regularizacijska tehnika zvana dropout i sposobnost stvaranja novih primjera deformiranjem slike

Budućnost dubokog učenja

- Nenadgledano učenje biti će slijedeći korak u razvoju dubokog učenja, koje je do sada bilo zasjenjeno sa strane nadgledanog učenja.

- Očekuje se sve veća kombinacija dubokog i podržanog (reinforcement) učenja.

Lokalna receptivna polja:
	- svaki skriveni neuron nije spojen sa svim ulaznim pixelima, već samo dijelom piksela
	- neki skriveni neuron može pokazivati na dijelove ulazne slike, npr. 3x3, 4x4 ili 5x5 (ili neke druge dimenzije) piksela iz ulazne slike
	- taj dio ulazne slike se naziva lokalno receptivno polje za skriveni neuron. Taj skriveni neuron će pokušati naučiti analizirati to specifično receptivno polje
	- na taj način skriveni sloj neće imati jednak broj neurona kao ulazni podaci, niti će biti u potpunosti povezani za svim pikselima već samo s njemu određenim poljima
	- Takav prozor ulaznih piksela se pomiče kroz sliku i mapira na ostale skrivene neurone
	- na taj način imamo puno manje težina i veza, odnosno puno manje parametara

Dijeljene težine
	- Za sve neurone u skrivenom sloju koriste se iste težine i pristranost (engl. bias)
	- dimenzije težina su iste kao i dimenzije lokalnog polja na ulaznoj slici
	- to znači da će neuroni u skrivenom sloju detektirati istu značajku, ali na drugim lokacijama na slici
	- poanta dijeljenih težina jest ta da ako skriveni neuron detektira neke rubove na specifičnom lokalnom polju, vrlo je vjerojatno da će i neki drugi skriveni neuron htjeti detektirati rubove u svojem specifičnom lokalnom polju
	- Drugim riječima, ako postoji slika psa te je pomaknemo u neku stranu, ili je zarotiramo to je još uvijek slika psa
	- takva poveznica između ulaznog sloja i prvog skrivenog sloja se često naziva mapa značajki
	- velika prednost djeljenja težina jest smanjen broj parametara. Jedna mapa značajki treba 5x5 (odnosno dimenzije lokalnog polja) + 1 parametara (pristranost)
	- dakle za mapa značajki, to je jednako 10 x 26 = 260 parametara, za potpuno povezani prvi sloj, to bi bilo sveukupno 28 x 28 (ulazne dimenzije) x 28 (proizvoljan broj skrivenih neurona) što je puno više

Sloj sažimanja (pooling layer)
	- oni se nalaze uglavnom odmah iza konvolucijskih slojeva
	- pojednostavjuju informacije s izlaznih slojeva konvolucijskog sloja
	- uzimaju izlaz iz mape značajka, dakle iz prvog skrivenog sloja te sažimu informacije iz neke manje regije (npr. 2x2) u jedan neuron 
	- učestala procedura za sažimanje jest max-pooling (na hrvatskom je to maksimalno sažimanje) koje uzima maksimalnu vrijednost iz te manje regije
	- Sloj sažimanja rezultira još manjem broju neuronu od onog u prvom skrivenom sloju
	STAVIT SLIKU IZ neuralnetworksanddeeplearning.com di su prikazani i konv layer i pooling layer
	- max pooling možemo zamislit kao način na koji mreža pita je li pronađena neka značajke negdje u regiji na slici
	- postoje ostali pristupi osim max poolinga, poput mean-pooling, koji uzima prosjek, L2 pooling, koji uzima korijen sume svih kvadrata unutar 2x2 regije na slici i slično

Način optimiranja težini se vrši jednako kao i u običnim neuronskim mrežama, stohastičkim (ili grupnim) gradijentnim spustom, no ima nekoliko razlika u samom izvršavanju algoritma pošto ova mreža više nije potpuno povezana.
