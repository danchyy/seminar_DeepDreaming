{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = 'mnist_log'\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard functions as seen in normal mnist work    \n",
    "def weight_variable(shape):\n",
    "    # Because of reLU activations, we will initialize weights as positive\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attaches summaries to a Tensor\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(var - mean))\n",
    "        tf.summary.scalar('stdev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var)) # max along tensor\n",
    "        tf.summary.scalar('min', tf.reduce_min(var)) # min in tensor\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # This ensures logical grouping of layers in the graph\n",
    "    with tf.name_scope(layer_name):\n",
    "        # This variable will hold the state of weights in layer\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # importing data\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    # Placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "    \n",
    "    with tf.name_scope('input_reshape'):\n",
    "        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', image_shaped_input, 10)\n",
    "    \n",
    "    # create simple network\n",
    "    \n",
    "    # first fully connected layer\n",
    "    hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "    \n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "    \n",
    "    # second layer\n",
    "    y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "    \n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        # The raw formulation of cross-entropy,\n",
    "        #\n",
    "        # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "        #                               reduction_indices=[1]))\n",
    "        #\n",
    "        # can be numerically unstable.\n",
    "        #\n",
    "        # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "        # raw outputs of the nn_layer above, and then average across\n",
    "        # the batch.\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits=y)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.reduce_mean(diff)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    merged = tf.summary.merge_all() # merges all the summaries thus far\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    \n",
    "    def feed_dict(train):\n",
    "        \"\"\"Make a TensorFlow feed_dict, map the data onto the placeholders.\n",
    "        Placeholders for this case are x, y_, keep_prob\"\"\"\n",
    "        if train:\n",
    "            xs, ys = mnist.train.next_batch(100, fake_data=False)\n",
    "            k = 0.5\n",
    "        else:\n",
    "            xs, ys = mnist.test.images, mnist.test.labels\n",
    "            k = 1.0\n",
    "        \n",
    "        return {x: xs, y_: ys, keep_prob: k}\n",
    "    \n",
    "    # Train the model, and write summaries\n",
    "    # Every 10th step, measure test-set accuracy, write test summaries\n",
    "    # All other steps, run train_step on training data & add summaries\n",
    "    for i in range(2000):\n",
    "        if i % 10 == 0:\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s : %s' % (i, acc))\n",
    "        else:\n",
    "            if i % 100 == 99:\n",
    "                # record execution status\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                     feed_dict = feed_dict(True),\n",
    "                                     options = run_options,\n",
    "                                     run_metadata = run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:\n",
    "                #record summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 0 : 0.1404\n",
      "Accuracy at step 10 : 0.5984\n",
      "Accuracy at step 20 : 0.7897\n",
      "Accuracy at step 30 : 0.83\n",
      "Accuracy at step 40 : 0.8459\n",
      "Accuracy at step 50 : 0.8449\n",
      "Accuracy at step 60 : 0.8768\n",
      "Accuracy at step 70 : 0.8901\n",
      "Accuracy at step 80 : 0.883\n",
      "Accuracy at step 90 : 0.8929\n",
      "Adding run metadata for 99\n",
      "Accuracy at step 100 : 0.9033\n",
      "Accuracy at step 110 : 0.9056\n",
      "Accuracy at step 120 : 0.9082\n",
      "Accuracy at step 130 : 0.9115\n",
      "Accuracy at step 140 : 0.9171\n",
      "Accuracy at step 150 : 0.9153\n",
      "Accuracy at step 160 : 0.9221\n",
      "Accuracy at step 170 : 0.9224\n",
      "Accuracy at step 180 : 0.9216\n",
      "Accuracy at step 190 : 0.9233\n",
      "Adding run metadata for 199\n",
      "Accuracy at step 200 : 0.9247\n",
      "Accuracy at step 210 : 0.9309\n",
      "Accuracy at step 220 : 0.929\n",
      "Accuracy at step 230 : 0.9293\n",
      "Accuracy at step 240 : 0.9335\n",
      "Accuracy at step 250 : 0.9319\n",
      "Accuracy at step 260 : 0.9322\n",
      "Accuracy at step 270 : 0.9339\n",
      "Accuracy at step 280 : 0.9347\n",
      "Accuracy at step 290 : 0.9355\n",
      "Adding run metadata for 299\n",
      "Accuracy at step 300 : 0.9395\n",
      "Accuracy at step 310 : 0.9399\n",
      "Accuracy at step 320 : 0.939\n",
      "Accuracy at step 330 : 0.9395\n",
      "Accuracy at step 340 : 0.9415\n",
      "Accuracy at step 350 : 0.9409\n",
      "Accuracy at step 360 : 0.9377\n",
      "Accuracy at step 370 : 0.9412\n",
      "Accuracy at step 380 : 0.94\n",
      "Accuracy at step 390 : 0.9424\n",
      "Adding run metadata for 399\n",
      "Accuracy at step 400 : 0.9422\n",
      "Accuracy at step 410 : 0.9435\n",
      "Accuracy at step 420 : 0.9437\n",
      "Accuracy at step 430 : 0.9466\n",
      "Accuracy at step 440 : 0.9475\n",
      "Accuracy at step 450 : 0.9463\n",
      "Accuracy at step 460 : 0.9473\n",
      "Accuracy at step 470 : 0.9443\n",
      "Accuracy at step 480 : 0.9473\n",
      "Accuracy at step 490 : 0.9481\n",
      "Adding run metadata for 499\n",
      "Accuracy at step 500 : 0.9486\n",
      "Accuracy at step 510 : 0.9504\n",
      "Accuracy at step 520 : 0.945\n",
      "Accuracy at step 530 : 0.951\n",
      "Accuracy at step 540 : 0.95\n",
      "Accuracy at step 550 : 0.9504\n",
      "Accuracy at step 560 : 0.9504\n",
      "Accuracy at step 570 : 0.95\n",
      "Accuracy at step 580 : 0.9518\n",
      "Accuracy at step 590 : 0.9515\n",
      "Adding run metadata for 599\n",
      "Accuracy at step 600 : 0.9517\n",
      "Accuracy at step 610 : 0.9506\n",
      "Accuracy at step 620 : 0.9487\n",
      "Accuracy at step 630 : 0.9528\n",
      "Accuracy at step 640 : 0.9544\n",
      "Accuracy at step 650 : 0.9549\n",
      "Accuracy at step 660 : 0.9543\n",
      "Accuracy at step 670 : 0.9538\n",
      "Accuracy at step 680 : 0.9558\n",
      "Accuracy at step 690 : 0.9558\n",
      "Adding run metadata for 699\n",
      "Accuracy at step 700 : 0.9544\n",
      "Accuracy at step 710 : 0.9558\n",
      "Accuracy at step 720 : 0.9553\n",
      "Accuracy at step 730 : 0.9549\n",
      "Accuracy at step 740 : 0.9566\n",
      "Accuracy at step 750 : 0.9556\n",
      "Accuracy at step 760 : 0.9576\n",
      "Accuracy at step 770 : 0.958\n",
      "Accuracy at step 780 : 0.9569\n",
      "Accuracy at step 790 : 0.9568\n",
      "Adding run metadata for 799\n",
      "Accuracy at step 800 : 0.9578\n",
      "Accuracy at step 810 : 0.9578\n",
      "Accuracy at step 820 : 0.9592\n",
      "Accuracy at step 830 : 0.9595\n",
      "Accuracy at step 840 : 0.9589\n",
      "Accuracy at step 850 : 0.9592\n",
      "Accuracy at step 860 : 0.9598\n",
      "Accuracy at step 870 : 0.9588\n",
      "Accuracy at step 880 : 0.9598\n",
      "Accuracy at step 890 : 0.9609\n",
      "Adding run metadata for 899\n",
      "Accuracy at step 900 : 0.9609\n",
      "Accuracy at step 910 : 0.9601\n",
      "Accuracy at step 920 : 0.9592\n",
      "Accuracy at step 930 : 0.9601\n",
      "Accuracy at step 940 : 0.9615\n",
      "Accuracy at step 950 : 0.9608\n",
      "Accuracy at step 960 : 0.9621\n",
      "Accuracy at step 970 : 0.9619\n",
      "Accuracy at step 980 : 0.9627\n",
      "Accuracy at step 990 : 0.9622\n",
      "Adding run metadata for 999\n",
      "Accuracy at step 1000 : 0.9637\n",
      "Accuracy at step 1010 : 0.9638\n",
      "Accuracy at step 1020 : 0.9632\n",
      "Accuracy at step 1030 : 0.9649\n",
      "Accuracy at step 1040 : 0.9632\n",
      "Accuracy at step 1050 : 0.964\n",
      "Accuracy at step 1060 : 0.9633\n",
      "Accuracy at step 1070 : 0.9636\n",
      "Accuracy at step 1080 : 0.9646\n",
      "Accuracy at step 1090 : 0.9656\n",
      "Adding run metadata for 1099\n",
      "Accuracy at step 1100 : 0.9656\n",
      "Accuracy at step 1110 : 0.963\n",
      "Accuracy at step 1120 : 0.9644\n",
      "Accuracy at step 1130 : 0.966\n",
      "Accuracy at step 1140 : 0.966\n",
      "Accuracy at step 1150 : 0.9671\n",
      "Accuracy at step 1160 : 0.9661\n",
      "Accuracy at step 1170 : 0.9658\n",
      "Accuracy at step 1180 : 0.9656\n",
      "Accuracy at step 1190 : 0.9649\n",
      "Adding run metadata for 1199\n",
      "Accuracy at step 1200 : 0.9663\n",
      "Accuracy at step 1210 : 0.9657\n",
      "Accuracy at step 1220 : 0.967\n",
      "Accuracy at step 1230 : 0.9679\n",
      "Accuracy at step 1240 : 0.9683\n",
      "Accuracy at step 1250 : 0.9673\n",
      "Accuracy at step 1260 : 0.965\n",
      "Accuracy at step 1270 : 0.9663\n",
      "Accuracy at step 1280 : 0.967\n",
      "Accuracy at step 1290 : 0.9696\n",
      "Adding run metadata for 1299\n",
      "Accuracy at step 1300 : 0.9678\n",
      "Accuracy at step 1310 : 0.9682\n",
      "Accuracy at step 1320 : 0.9683\n",
      "Accuracy at step 1330 : 0.968\n",
      "Accuracy at step 1340 : 0.9691\n",
      "Accuracy at step 1350 : 0.9683\n",
      "Accuracy at step 1360 : 0.9667\n",
      "Accuracy at step 1370 : 0.9681\n",
      "Accuracy at step 1380 : 0.9692\n",
      "Accuracy at step 1390 : 0.9682\n",
      "Adding run metadata for 1399\n",
      "Accuracy at step 1400 : 0.9689\n",
      "Accuracy at step 1410 : 0.9688\n",
      "Accuracy at step 1420 : 0.9686\n",
      "Accuracy at step 1430 : 0.97\n",
      "Accuracy at step 1440 : 0.9694\n",
      "Accuracy at step 1450 : 0.9686\n",
      "Accuracy at step 1460 : 0.9689\n",
      "Accuracy at step 1470 : 0.969\n",
      "Accuracy at step 1480 : 0.9704\n",
      "Accuracy at step 1490 : 0.9698\n",
      "Adding run metadata for 1499\n",
      "Accuracy at step 1500 : 0.9703\n",
      "Accuracy at step 1510 : 0.9679\n",
      "Accuracy at step 1520 : 0.9702\n",
      "Accuracy at step 1530 : 0.9698\n",
      "Accuracy at step 1540 : 0.9695\n",
      "Accuracy at step 1550 : 0.9696\n",
      "Accuracy at step 1560 : 0.9692\n",
      "Accuracy at step 1570 : 0.9695\n",
      "Accuracy at step 1580 : 0.9728\n",
      "Accuracy at step 1590 : 0.9709\n",
      "Adding run metadata for 1599\n",
      "Accuracy at step 1600 : 0.971\n",
      "Accuracy at step 1610 : 0.9708\n",
      "Accuracy at step 1620 : 0.9699\n",
      "Accuracy at step 1630 : 0.972\n",
      "Accuracy at step 1640 : 0.9706\n",
      "Accuracy at step 1650 : 0.9715\n",
      "Accuracy at step 1660 : 0.9718\n",
      "Accuracy at step 1670 : 0.9719\n",
      "Accuracy at step 1680 : 0.9717\n",
      "Accuracy at step 1690 : 0.9714\n",
      "Adding run metadata for 1699\n",
      "Accuracy at step 1700 : 0.9704\n",
      "Accuracy at step 1710 : 0.9717\n",
      "Accuracy at step 1720 : 0.9728\n",
      "Accuracy at step 1730 : 0.9714\n",
      "Accuracy at step 1740 : 0.9698\n",
      "Accuracy at step 1750 : 0.9724\n",
      "Accuracy at step 1760 : 0.9688\n",
      "Accuracy at step 1770 : 0.9696\n",
      "Accuracy at step 1780 : 0.9692\n",
      "Accuracy at step 1790 : 0.97\n",
      "Adding run metadata for 1799\n",
      "Accuracy at step 1800 : 0.9719\n",
      "Accuracy at step 1810 : 0.9713\n",
      "Accuracy at step 1820 : 0.9717\n",
      "Accuracy at step 1830 : 0.972\n",
      "Accuracy at step 1840 : 0.9726\n",
      "Accuracy at step 1850 : 0.9723\n",
      "Accuracy at step 1860 : 0.972\n",
      "Accuracy at step 1870 : 0.972\n",
      "Accuracy at step 1880 : 0.9714\n",
      "Accuracy at step 1890 : 0.9709\n",
      "Adding run metadata for 1899\n",
      "Accuracy at step 1900 : 0.9721\n",
      "Accuracy at step 1910 : 0.9726\n",
      "Accuracy at step 1920 : 0.9726\n",
      "Accuracy at step 1930 : 0.9723\n",
      "Accuracy at step 1940 : 0.9699\n",
      "Accuracy at step 1950 : 0.9706\n",
      "Accuracy at step 1960 : 0.9717\n",
      "Accuracy at step 1970 : 0.9713\n",
      "Accuracy at step 1980 : 0.971\n",
      "Accuracy at step 1990 : 0.9725\n",
      "Adding run metadata for 1999\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(LOG_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "\n",
    "tf.gfile.MakeDirs(LOG_DIR)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
